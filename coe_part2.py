# -*- coding: utf-8 -*-
"""coe_part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MUDg2Eb_ZM1rKxY9seJjsRaeKmTKG6Yz
"""

!pip install pdfplumber

!pip install pdf2image
!apt-get install poppler-utils -y

!apt-get install tesseract-ocr -y
!pip install pytesseract

import pdfplumber
ocr = False
with pdfplumber.open("/content/BOS_TY_EXTC_Syllabus.pdf") as pdf:
    first_page = pdf.pages[0]
    if not first_page:
        ocr = True

import pdfplumber
import pytesseract
from pdf2image import convert_from_path

pdf_path = "/content/BOS_TY_EXTC_Syllabus.pdf"
pages_text = []

if not ocr:  # digital PDF
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            pages_text.append({
                "page": i,
                "text": text,
                "source_file": pdf_path,
                "ocr": False
            })
else:  # scanned PDF
    pages = convert_from_path(pdf_path, dpi=300)
    for i, page in enumerate(pages, start=1):
        text = pytesseract.image_to_string(page, lang='eng+hin+ben+chi_sim')
        pages_text.append({
            "page": i,
            "text": text,
            "source_file": pdf_path,
            "ocr": True
        })

!pip install langdetect

#preprocessing the data
from langdetect import detect

for page in pages_text:
    if page["text"]:
        page["language"] = detect(page["text"])

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
chunk_size = 400   # tokens per chunk
overlap = 80       # tokens to overlap between chunks
chunks = []

for page in pages_text:
    tokens = tokenizer.encode(page["text"])
    i = 0
    while i < len(tokens):
        chunk_tokens = tokens[i:i+chunk_size]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append({
            "chunk_text": chunk_text,
            "page": page["page"],
            "source_file": page["source_file"],
            "language": page.get("language", "unknown")
        })
        i += (chunk_size - overlap)

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_texts = [c["chunk_text"] for c in chunks]

embeddings = model.encode(chunk_texts, batch_size=64, show_progress_bar=True)

!pip install faiss-cpu

import faiss

d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(embeddings)
print(f"Stored {index.ntotal} chunks in index.")

def search(query, top_k=3):
    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            "rank": i+1,
            "text": chunks[idx]["chunk_text"],
            "page": chunks[idx]["page"],
            "source_file": chunks[idx]["source_file"],
            "distance": float(distances[0][i])
        })
    return results

# Example search
query = "What are the subjects in 5th semester?"
results = search(query, top_k=3)
for r in results:
    print(f"Rank {r['rank']} (Page {r['page']}): {r['text'][:200]}...\n")

!pip install transformers accelerate bitsandbytes

pip install -U bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id, token="Hugging face token *")

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    token="Hugging face token *",
    device_map="auto",
    torch_dtype="auto"  # uses FP16 if GPU available
)

from transformers import pipeline

sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

def search(query, top_k=3):
    query_vec = sentence_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            "rank": i+1,
            "text": chunks[idx]["chunk_text"],
            "page": chunks[idx]["page"],
            "source_file": chunks[idx]["source_file"],
            "distance": float(distances[0][i])
        })
    return results

llm_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

def rag_answer(query, top_k=3, max_new_tokens=300):
    retrieved = search(query, top_k)
    context = "\n\n".join([r["text"] for r in retrieved])

    prompt = f"""
    You are a helpful assistant.
    Use the following context to answer the question.
    If the answer is not in the context, say you don't know.

    Context:
    {context}

    Question: {query}
    Answer:
    """

    response = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)
    return response[0]["generated_text"]

query = "What are the subjects in 5th semester?"
answer = rag_answer(query, top_k=3)
print(answer)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id, token="token*")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    token="token*",
    device_map="auto",
    torch_dtype="auto"
)

from transformers import pipeline

sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

def search(query, top_k=3):
    query_vec = sentence_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            "rank": i+1,
            "text": chunks[idx]["chunk_text"],
            "page": chunks[idx]["page"],
            "source_file": chunks[idx]["source_file"],
            "distance": float(distances[0][i])
        })
    return results

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

query = "What are the subjects in 5th semester?"
search_results = search(query, top_k=3)
context = " ".join([r['text'] for r in search_results])


answer = qa_pipeline(f"Context: {context}\nQuestion: {query}\nAnswer:", max_new_tokens=150)
print(answer[0]['generated_text'])